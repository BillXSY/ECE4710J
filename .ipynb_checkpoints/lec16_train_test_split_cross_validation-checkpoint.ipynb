{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split and Cross Validation\n",
    "\n",
    "In this section we will work through the train test-split and the process of cross validation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.4.2.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.4.2.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# !pip install cufflinks\n",
    "import plotly.offline as py\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.figure_factory as ff\n",
    "import cufflinks as cf\n",
    "cf.set_config_file(offline=True, sharing=False, theme='ggplot');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data\n",
    "\n",
    "For this notebook, we will use the seaborn `mpg` dataset which describes the fuel mileage (measured in miles per gallon or mpg) of various cars along with characteristics of those cars.  Our goal will be to build a model that can predict the fuel mileage of a car based on the characteristics of that car."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seaborn import load_dataset\n",
    "data = load_dataset(\"mpg\")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data has some rows with missing data. We will ignore those rows until later for the sake of this lecture. We can use the Pandas `DataFrame.isna` function to find rows with missing values and drop them, although of course, this is not always the best idea!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[~data.isna().any(axis=1)].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split\n",
    "\n",
    "The first thing we will want to do with this data is construct a train/test split. Constructing a train test split before EDA and data cleaning can often be helpful.  This allows us to see if our data cleaning and any conclusions we draw from visualizations generalize to new data. This can be done by re-running the data cleaning and EDA process on the test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Pandas Operations\n",
    "\n",
    "We can sample the entire dataset to get a permutation and then select a range of rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_data = data.sample(frac = 1, random_state = 42)\n",
    "shuffled_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting a range of rows for training and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_point = int(shuffled_data.shape[0] * 0.90)\n",
    "split_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = shuffled_data.iloc[:split_point]\n",
    "te = shuffled_data.iloc[split_point:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tr), len(te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "te.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking that they add up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tr) + len(te) == len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffling with Numpy\n",
    "\n",
    "We can directly shuffle the data with `numpy`, and then select the corresponding rows from our original DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(100)\n",
    "shuffled_indices = np.random.permutation(np.arange(len(data)))\n",
    "shuffled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.iloc[shuffled_indices].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = data.iloc[shuffled_indices[:split_point]]\n",
    "te = data.iloc[shuffled_indices[split_point:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tr), len(te)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using SKLearn\n",
    "\n",
    "We can use the `train_test_split` function from `sklearn.model_selection` to do this easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr, te = train_test_split(data, test_size = 0.1, random_state = 83)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tr), len(te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "te.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building A Basic Model\n",
    "\n",
    "Let's go through the process of building a model. Let's start by looking at the raw quantitative features available. We will first use just our own feature function (as we did in previous lectures). This function will just extract the quantitative features we can use for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_design_matrix(df):\n",
    "    X = df[[\"cylinders\", \"displacement\", \n",
    "          \"horsepower\", \"weight\", \"acceleration\", \"model_year\"]]\n",
    "    return X\n",
    "\n",
    "basic_design_matrix(tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we fit a `scikit-learn` `LinearRegression` model to our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(basic_design_matrix(tr), tr['mpg'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the error we will use the **Root Mean Squared Error (RMSE)** which is like the mean squared error but in the correct units (mpg) instead of (mpg^2). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y, yhat):\n",
    "    return np.sqrt(np.mean((y - yhat)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training error is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_hat = model.predict(basic_design_matrix(tr))\n",
    "Y = tr['mpg']\n",
    "print(\"Training Error (RMSE):\", rmse(Y, Y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing Error\n",
    "\n",
    "(Don't try this at home!)  \n",
    "\n",
    "**Notice:** The test error is slightly higher than the training error. This is typically (but not always) the case. Sometimes we get lucky and the test data is \"easier to predict\" or happens to closely follow the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y_hat = model.predict(basic_design_matrix(te))\n",
    "# Y = te['mpg']\n",
    "# print(\"Testing Error (RMSE):\", rmse(Y, Y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Keeping track of all the models.\n",
    "\n",
    "In this notebook (and in life) we will want to keep track of all our models. \n",
    "\n",
    "We store our model settings in a dictionary. The key is some identifying name, and the value is a 2-element tuple, with the first element being the transformation function (e.g. `basic_design_matrix`), and the second element being an empty model object (e.g. `LinearRegression()`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\"quant\": (basic_design_matrix, LinearRegression())}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Feature Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we might want to look at the displacement per cylinder as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dispcyl_design_matrix(df):\n",
    "    X = basic_design_matrix(df)\n",
    "    X['displacement/cylinder'] = X['displacement'] / X['cylinders']\n",
    "    return X\n",
    "\n",
    "dispcyl_design_matrix(tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can build a linear model using the same quantitative features as before, but with this new \"displacement per cylinder\" feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(dispcyl_design_matrix(tr), tr['mpg'])\n",
    "\n",
    "models['quant+dc'] = (dispcyl_design_matrix, LinearRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_hat = model.predict(dispcyl_design_matrix(tr))\n",
    "Y = tr['mpg']\n",
    "print(\"Training Error (RMSE):\", rmse(Y, Y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our training error is definitely lower than with the previous model, but what we really care about is the model's performance on new data. But we shouldn't actually ever look at the test data. Instead, to compare these models, we can use cross-validation to \"mimic\" the train-test split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following function we use the sklearn `KFold` cross validation class. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define a five fold cross validation with \n",
    "\n",
    "```python \n",
    "five_fold = KFold(n_splits=5)\n",
    "```\n",
    "\n",
    "Then we loop over the 5 splits and get the indicies (`tr_ind`) in the training data to use for training and the indices (`va_ind`) in the training data to use for validation:\n",
    "\n",
    "```python\n",
    "for tr_ind, te_ind in five_fold.split(tr):\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.base import clone\n",
    "\n",
    "def cross_validate_rmse(phi_function, model):\n",
    "    model = clone(model)\n",
    "    five_fold = KFold(n_splits = 5, random_state = 100, shuffle = True)\n",
    "    rmse_values = []\n",
    "    for tr_ind, va_ind in five_fold.split(tr):\n",
    "        \n",
    "        X_train = phi_function(tr.iloc[tr_ind, :])\n",
    "        y_train = tr['mpg'].iloc[tr_ind]\n",
    "        X_val = phi_function(tr.iloc[va_ind, :])\n",
    "        y_val = tr['mpg'].iloc[va_ind]\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        rmse_values.append(rmse(y_val, model.predict(X_val)))\n",
    "        \n",
    "    return np.mean(rmse_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Valiating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cross_validate_rmse(dispcyl_design_matrix, LinearRegression())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following helper function generates a plot comparing all the models in the `transformations` dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(models):\n",
    "    \n",
    "    # Compute the training error for each model\n",
    "    training_rmse = []\n",
    "    for transformation, model in models.values():\n",
    "        model = clone(model)\n",
    "        model.fit(transformation(tr), tr['mpg'])\n",
    "        training_rmse.append(rmse(tr['mpg'], model.predict(transformation(tr))))\n",
    "    \n",
    "    # Compute the cross validation error for each model\n",
    "    validation_rmse = [cross_validate_rmse(transformation, model) for transformation, model in models.values()]\n",
    "    \n",
    "    names = list(models.keys())\n",
    "    fig = go.Figure([\n",
    "        go.Bar(x = names, y = training_rmse, name=\"Training RMSE\"),\n",
    "        go.Bar(x = names, y = validation_rmse, name=\"CV RMSE\")])\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = compare_models(models)\n",
    "fig.update_yaxes(range = [0, 4], title = \"RMSE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So not only did the new displacement / cylinders feature improve our training error, it also improved our cross-validation error. This indicates that this feature helps our model generalize, or in other words, that it has \"predictive power.\"\n",
    "\n",
    "Now let's try adding some categorical data, such as the `origin` column. As this is categorical data, we will have to one-hot encode this variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['origin'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately, it looks like we have only three possible values for `origin`. We will use `scikit-learn`'s one-hot encoder to do the transformation. Check out Lecture 14 for a refresher on how this works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "oh_enc = OneHotEncoder()\n",
    "oh_enc.fit(data[['origin']])\n",
    "\n",
    "def origin_design_matrix(df):\n",
    "    X = dispcyl_design_matrix(df)\n",
    "    ohe_cols = pd.DataFrame(oh_enc.transform(df[['origin']]).todense(), \n",
    "                           columns = oh_enc.get_feature_names(),\n",
    "                           index = df.index)\n",
    "    return X.join(ohe_cols)\n",
    "\n",
    "models['quant+dc+o'] = (origin_design_matrix, LinearRegression())\n",
    "\n",
    "origin_design_matrix(tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = compare_models(models)\n",
    "fig.update_yaxes(range = [0, 4], title = \"RMSE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like adding these new features about origin didn't really affect our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try if we can gain any information from the `name` column. This column contains the make and model of each car. The models are fairly unique, so let's try to extract information about the brand (e.g. `ford`). The following cell shows the top 20 words that appear in this column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr['name'].str.split().explode().value_counts().head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like there is at least one model here (`corolla`), but it does show the most common brands. We will add one column for each of these strings, with a `1` for a specific car indicating that the name of the car contains the string.\n",
    "\n",
    "Note: In practice, you would use `scikit-learn` or some other package, but we will do this manually just to be explicit about what we're doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brands = tr['name'].str.split().explode().value_counts().head(20).index\n",
    "\n",
    "def brands_design_matrix(df):\n",
    "    X = origin_design_matrix(df)\n",
    "    for brand in brands:\n",
    "        X[brand] = df['name'].str.contains(brand, regex = False).astype(float)\n",
    "    return X\n",
    "\n",
    "models['quant+dc+o+b'] = (brands_design_matrix, LinearRegression())\n",
    "\n",
    "brands_design_matrix(tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = compare_models(models)\n",
    "fig.update_yaxes(range = [0, 4], title = \"RMSE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding the brand information to our design matrix decreased our training error, but it increased our cross-validation error. Looks like we overfit!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "\n",
    "In this section we explore the use of regularization techniques to address overfitting.\n",
    "\n",
    "### Ridge Regression\n",
    "\n",
    "Ridge regression combines the ridge (L2, Squared) regularization function with the least squares loss.  \n",
    "\n",
    "$$\n",
    "\\hat{\\theta}_\\alpha = \\arg \\min_\\theta \\left[ \\left(\\frac{1}{n} \\sum_{i=1}^n \\left(Y_i -  f_\\theta(X_i)\\right)^2 \\right) + \\alpha \\sum_{k=1}^d \\theta_k^2 \\right]\n",
    "$$\n",
    "\n",
    "Ridge regression, like ordinary least squares regression, also has a closed form solution for the optimal $\\hat{\\theta}_\\alpha$\n",
    "\n",
    "$$\n",
    "\\hat{\\theta}_\\alpha = \\left(X^T X + n \\alpha \\mathbf{I}\\right)^{-1} X^T Y\n",
    "$$\n",
    "\n",
    "where $\\mathbf{I}$ is the identity matrix, $n$ is the number of data points, and $\\alpha$ is the regularization hyperparameter.\n",
    "\n",
    "Notice that even if $X^T X$ is not full rank, the addition of $n \\alpha \\mathbf{I}$ (which is full rank) makes $\\left(X^T X + n \\alpha \\mathbf{I}\\right)$ invertible. Thus, ridge regression addresses the possible issue of having an underdetermined system and partially improves the numerical stability of the solution.\n",
    "\n",
    "### The Regularization Hyperparameter\n",
    "\n",
    "The $\\alpha$ parameter is our regularization **hyperparameter**. It is a hyperparameter because it is not a model parameter but a choice of how we want to balance fitting the data and \"over-fitting\". The goal is to find a value of this hyper\"parameter to maximize our accuracy on the **test data**. However, **we can't use the test data to make modeling decisions** so we turn to cross validation. The standard way to find the best $\\alpha$ is to try a bunch of values (perhaps using binary search) and take the one with the lowest cross validation error. \n",
    "\n",
    "You may have noticed that in the video lecture we use $\\lambda$ instead of $\\alpha$.  This is because many textbooks use $\\lambda$ and sklearn uses $\\alpha$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression in SK Learn\n",
    "\n",
    "Both Ridge Regression and Lasso are built-in functions in SKLearn.  Let's start by importing the `Ridge` Regression class which behaves identically to the `LinearRegression` class we used earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the documentation.  Notice the regularized loss function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ridge?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of just looking at the top 20 brands, let's bag-of-words encode the entire name column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(tr['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def name_design_matrix(df):\n",
    "    X = origin_design_matrix(df)\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    X[feature_names] = vectorizer.transform(df['name']).toarray()\n",
    "    return X\n",
    "\n",
    "name_design_matrix(tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_validate_rmse(name_design_matrix, LinearRegression())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woah, our RMSE is really, really large! This is due to the fact that by adding all of these columns, our design matrix is no longer full rank. `numpy` tried to take the inverse of $\\mathbb{X}^T \\mathbb{X}$, but it ended up sending the parameters to really extreme values, leading to really extreme predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get around this, let's try regularization. As we're introducing regularization, let's also standardize our quantitative features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "quantitative_features = [\"cylinders\", \"displacement\", \"horsepower\", \"weight\", \"acceleration\", \"model_year\"]\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(basic_design_matrix(tr[quantitative_features]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def name_design_matrix_std(df):\n",
    "    X = name_design_matrix(df)\n",
    "    X[quantitative_features] = scaler.transform(X[quantitative_features])\n",
    "    return X\n",
    "\n",
    "name_design_matrix_std(tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_validate_rmse(name_design_matrix_std, Ridge(alpha = .5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks more like it. Let's add this model into our set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models['quant+dc+o+n-Ridge.5'] = (name_design_matrix_std,  Ridge(alpha = .5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = compare_models(models)\n",
    "fig.update_yaxes(range = [0, 4], title = \"RMSE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how our training error dropped significantly, but our CV error only changed a little bit. Let's try a different value of $\\alpha$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models['quant+dc+o+n-Ridge100'] = (name_design_matrix_std, Ridge(alpha = 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = compare_models(models)\n",
    "fig.update_yaxes(range = [0, 4], title = \"RMSE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oops, that was too much regularization. Let's do a search to find the best $\\alpha$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = np.linspace(.5, 3.5, 20)\n",
    "cv_values = []\n",
    "train_values = []\n",
    "for alpha in alphas:\n",
    "    model = Ridge(alpha = alpha)\n",
    "    model.fit(name_design_matrix_std(tr), tr['mpg'])\n",
    "    train_values.append(rmse(tr['mpg'], model.predict(name_design_matrix_std(tr))))\n",
    "    \n",
    "    validation_rmse = cross_validate_rmse(name_design_matrix_std, model)\n",
    "    cv_values.append(validation_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x = alphas, y = train_values, mode=\"lines+markers\", name=\"Train\"))\n",
    "fig.add_trace(go.Scatter(x = alphas, y = cv_values, mode=\"lines+markers\", name=\"CV\"))\n",
    "fig.update_layout(xaxis_title=r\"$\\alpha$\", yaxis_title=\"CV RMSE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models['quant+dc+o+n-Ridge1.75'] = (name_design_matrix_std,  Ridge(alpha = 1.75))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = compare_models(models)\n",
    "fig.update_yaxes(range = [0, 4], title = \"RMSE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models['quant+dc+o+n-RidgeCV'] = (name_design_matrix_std, RidgeCV())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = compare_models(models)\n",
    "fig.update_yaxes(range = [0, 4], title = \"RMSE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso Regression\n",
    "\n",
    "Lasso regression combines the absolute (L1) regularization function with the least squares loss.  \n",
    "\n",
    "$$\n",
    "\\hat{\\theta}_\\alpha = \\arg \\min_\\theta \\left(\\frac{1}{n} \\sum_{i=1}^n \\left(Y_i -  f_\\theta(X_i)\\right)^2 \\right) + \\alpha \\sum_{k=1}^d |\\theta_k|\n",
    "$$\n",
    "\n",
    "Lasso is actually an acronym (and a cool name) which stands for Least Absolute Shrinkage and Selection Operator.  It is an absolute operator because it is the absolute value.  It is a shrinkage operator because it favors smaller parameter values.  It is a selection operator because it has the peculiar property of pushing parameter values all the way to zero thereby selecting the remaining features.  It is this last property that makes Lasso regression so useful. By using Lasso regression and setting sufficiently large value of $\\alpha$ you can eliminate features that are not informative. \n",
    "\n",
    "Unfortunately, there is no closed form solution for Lasso regression and so iterative optimization algorithms like gradient descent are typically used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso, LassoCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models['quant+dc+o+n-LassoCV'] = (name_design_matrix_std,  LassoCV())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = compare_models(models)\n",
    "fig.update_yaxes(range = [0, 4], title = \"RMSE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the distribution of the parameters for both the `RidgeCV` and `LassoCV` models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RidgeCV()\n",
    "model.fit(name_design_matrix_std(tr), tr['mpg'])\n",
    "ridge_coef = model.coef_\n",
    "\n",
    "model = LassoCV()\n",
    "model.fit(name_design_matrix_std(tr), tr['mpg'])\n",
    "lasso_coef = model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff.create_distplot([ridge_coef, lasso_coef], [\"Ridge\", \"Lasso\"], bin_size = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_design_matrix_std(tr).columns[lasso_coef > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
